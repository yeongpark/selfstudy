# selfstudy

####proc sql 기본 구문(160529)
>proc sql;

>select

>from

>where

>group by

>order by

 
- proc sql; <- sql을 시작하기 위한 구문
- select 절은 data step에서 input 구문과 비슷하게, 변수를 지정해주는 것
- from 절은 data step에서 set 구문과 비슷하게 어느 데이터 셋에서 불러올지를 정하는 것
- where 절은 하나 이상의 조건으로 from에서 불러올 때, 부분집합을 만들어주는 것 data step에서 where 과 같음
- group by 절은 특정 변수에 따라 그룹을 나누는 것
- order by 절은 특정 변수에 따라 최종 쿼리를 정렬하는 것
 
다른 프로시져와는 달리, sql은 절의 순서가 중요해서, 위와 같은 순서를 유지해야함
select 와 from 절은 항상 존재해야 하며, where/group by/order by 절들은 옵션형태를 가짐
 
####<테이블을 쿼리하는 기본 예제>
>proc sql;

>select empid, jobcode, salary, salary *.06 as bonus

>from sasuser.payrollmaster

>where salary<32000

>order by jobcode;

>quit;



proc sql로 시작하고, from 절에서 지정된 sasuser.payrollmaster 라는 데이터셋에서 where 절에서 지정된 조건 salary 가 32000 보다 작은 관찰치만을 불러와서, select 절에서 지정된, empid, jobcode, salary, bonus를 만드는데, bonus는 salary에 .06을 곱한 값, 다음과 같은 형태를 alias 라고 부름 출력에는 empid, jobcode, salary, bonus 만이 나옴 마지막으로 결과값은 jobcode에 따라 정렬함

----------
####통계의 힘(160601)


#### <i class="icon-pencil"></i> 프롤로그

통계학을 왜 공부해야 하는가?
: 파악, 예측, 통찰
- 데이터를 통계적으로 분석해 어떤 업종이나 업무에서든 개인 혹은 집단 차원에서 인간의 행동을 통찰 -> 그것을 조금이나마 변화시켜 비즈니스 이익을 높이려함

통계학은 도메인별로 사용되는 방법이 다름 
- 농업/공업에서는 실험설계, 문자/음성/자연어 처리를 자동화하는 머신러닝 등

또한, 분석하려는 목적에 따라 통계학을 세 가지로 분류함
: 현상 파악, 인과관계 통찰, 미래 예측 -> 책에서는 인과관계 통찰을 주로 이야기 함

>현상파악 : 마케팅 조사에서 “현재 특정 제품을 사용하는 사람이 몇 명인지 정확하게 추정하는 것” 등을 말함. 조사를 통해 추정되는 평균과 비율을 그래프에 정리하는 작업은 대다수 비즈니스 종사자들이 어렵지 않게 하고 있어 책에서 굳이 다루지 않음

>미래예측 : 앞으로 주가나 원자재 가격이 상승할지 혹은 재고가 어떻게 변할지 같은 것을 정확히 예측하려는 것. 또는 머신러닝에서의 음성의 인식 등 ‘인간이라면 어떻게 인식하는가’하는 부분을 데이터로 정확히 예측함으로써 인간의 인지를 모방하려는 사고방식도 여기에 해당. 이 책에서 이를 다루지 않는 이유는, 분석방법 운운하기 전에 그 밖의 여러 사정 때문에 실제 예측을 현실화하는 것이 매우 어렵기 때문.

>통찰의 통계학 : 마케팅에서는 ‘이 상품이 몇 개 팔릴까?’ 보단 ‘어떤 홍보를 해야 효과가 있을까?’, ‘어떻게 광고해야 히트상품이 될까?’를 더 중요시함. 즉, 그 인과관계를 찾아야 함


####2. 제 1장

2.1 통찰의 통계학에 필요한 세 가지 지식

: 평균 vs 비율
- continuous variable은 평균으로, categorical variable은 비율(해당하는 정도)로 나타냄

: 데이터는 점(평균 또는 비율)이 아니라 구간
- 그러나 한 점으로 표현하면 데이터의 많은 정보를 놓침
- 따라서 통계학은 ‘데이터는 대체로 어디에서 어디까지의 범위에 속해있는가’하는 식의 구간으로 파악하는 방법을 고안

: ‘무슨 값을 어떻게 정리해야 하는지’에 대한 이해
- 원인과 결과(outcome)를 잘 정의해야 함
- 비즈니스에서 가치 있는 데이터 분석은 ‘최대화하거나 최소화해야하는 항목’이 무엇인지 알아내는 것

: 설명변수의 우선순위 매기는 방법
- 인과관계가 너무 당연한 것이어서는 안된다
- 아웃컴에 미치는 영향이 명백하더라도 조절이 가능해야 한다
- 지금까지 그다지 주목 받지 않고 분석된 적이 별로 없어야 한다

2.2 인과관계 파악에 중요한 ‘평균’의 본질

- ‘현상파악’에서 데이터의 대푯값 : 평균, 중위수, 최빈값
- 통찰을 위한 통계학에서는 중앙값과 최빈수에 신경쓰는 일이 거의 없다
- 관측값과 참값의 차이(절댓값)을 최소로 만드는 추측값은 “중앙값”
- 그러나 절댓값 계산이 어려워 차이의 제곱을 최소로하는 추측값을 찾았는데 이게 “평균” by가우스
- 즉 평균은 최소제곱법에 기초하여 측정값에 포함되어 있는 차이를 가장 적게 만드는 뛰어난 추정값임

2.3 어떻게 평균으로 진실을 포착할 수 있는가
- 인과관계 통찰 관점에서 볼 때 평균값이 중앙값보다 관심이 있는 것에 대한 직접적인 대답이 되는 경우가 많다
- 인과관계 통찰할 때는 어떤 결과를 나타내는 값의 ‘총량’을 최대화하거나 최소화하느 쪽으로 관심이 쏠리게 마련인데 ‘뭔가의 요인을 바꾸면 결과값의 총량이 어떻게 변하는가’ 하는 부분에 중앙값은 속 시원한 대답을 못함
- 또한 왜 평균값을 불규칙성이 내포된 데이터의 배후에 있는 ‘참값’이라고 생각해도 되는지 가우스가 이야기 함
- 가우스의 질문 “평균값을 사용하는 것이 참값을 추정하는 좋은 방법이 되는 조건이란 무엇인가” 에서 정규분포라 불리는 ‘불규칙의 법칙성’에 이르게 됨
- 즉, 데이터의 불규칙성이 정규분포를 따르고 있으면 최소제곱법이 가장 좋은 추정 방법이고, 그 결과 평균값이 가장 좋은 추정값이 된다.

2.4 정규분포
- 좌우대칭인 종 모양의 매끄러운 곡선으로 표현되는 데이터의 불규칙성
- 그래프의 가로축은 변수, 세로축은 데이터 전체에서 차지하는 비율. 즉, 이 집단에서 임의로 뽑아낸 1명의 변수값이 어떠한가 하는 확률을 의미(밀도)

2.5 중심극한정리(CLT, Central Limit Theorem)
- 데이터가 정규분포를 따르지 않는다고 해도 ‘데이터 값을 거듭 추가할수록 정규분포에 수렴’ 
- 데이터 값을 몇개 추가한 것이 정규분포를 따르면, 거기에 다시 ‘추가한 데이터의 수’로 나눈 값인 평균값도 정규분포에 수렴함
그렇기에 ‘원시데이터의 불규칙성과 그것의 평균값’과 ‘원시데이터의 불규칙성과는 상관없는, 평균값 자체의 불규칙성’의 구별이 중요함

2.7 표준편차로 데이터의 대략적 범위를 알 수 있다
- 사분위수, 백분위수(현상파악에서 제시)
- 분산, 표준편차

#### 3. 제 2장

3.1 제 1종오류와 2종오류 사이에 놓인 “최강”:의 개념

- 통계적으로 유의미한 차이가 있다(statistically significant difference) = 우연한 불규칙성에 의해 나온 차이가 아니라, 어떤 요인에 의해 발생한 차이일 수 있다.
- 그렇다면 현실적으로(평균과 표준편차를 아는 것 만으로) 유의미한 차이가 있다고 말할 수 있는가? -> 그렇지 않다. “검정력”이라는 개념을 알아야 함
- 단지 그룹간 평균값에서 표준편차의 2배 이상이 벗어나 있으니 유의한 차이가 있다고 하는 것은 “검정력이 강하지 않은 것” ->늘 그 차이가 표준편차 2배 이상만큼 차이가 나야 하는 것은 아니므로, 주어진 상황에서 그 차이를 발견하는 것이 검정력을 높이는 것임.
- 검정력의 정확한 의미는 “어떤 차이가 존재하고 있다는 가설이 올바를 때, 유의차가 존재한다고 확실히 말할 수 있는 확률”임
- 그럼 검정력을 높이면 무조건 좋은가? 어떤 차이가 있다는 가설이 맞다면, 유의미한 차이는 100% 발견한다? 이렇게 되면 올바른 가설을 못 보고 놓칠 수도 있고, 차이가 없는 것을 차이가 있다고 얘기하게 될 수도 있다.
- 여기서 차이가 없는 것을 있다고 판단하는 경우를 “제 1종 오류”, 차이가 있는데 없다고 판단하는 경우를 “제 2종 오류”라고 한다.
제 1종오류와 2종오류 사이에서 어떻게 현실적으로 올바른 판단을 하는지를 공식처럼 정해주는 학문이 바로 “통계학”이다.
두 오류는 trade off 관계여서, 1종오류를 대부분 5%까지만 허용하고, 그 안에서 2종오류를 어디까지 최소화할지를 정하는데, 여기서 5%에 해당하는 1종오류의 허용 한계치를 “유의수준”이라 함
이처럼 설정된 유의수준 내에서 검정력이 가장 높은 분석방법을 “최강검정법(most powerful test, MPtest)”라고 함

3.2 “오차범위”와 유의미한 통찰을 위한 표본크기 설계

- 오차는 데이터의 수와 데이터의 불규칙성(분산, 표준편차 등)에 의해 결정됨
- 통계학적 의미의 오차 : 한정된 데이터를 통해 계산된 평균 또는 비율이 “참값”에서 어떤 정도와 확률로 벗어나 있는지를 나타내는 것
여기서 ‘어느정도 벗어날 수 있는가’가 데이터의 불규칙성과 관련된 것
- 정답은 평균의 표준오차를 구하는 것. 표본평균에 대해 표준편차를 계산한 것이 표준오차. 이것의 의미는? 표본평균처럼 sample을 통해 마땅히 얻어지는 값에 대한 분포에서의 표준편차가 표준오차가 되는 것. 표준편차는 원시데이터 그 자체의 불규칙성을 나타내는 지표임
다수의 데이터에서 얻어진 평균값의 불규칙성인 표준오차는 반드니 원시데이터의 불규칙성인 표준편차보다 작게 마련이며, sample size가 커질수록 표준오차가 작아짐.
이를 통해 어느정도의 표준오차를 허용하기 위해 어느정도의 표본 크기가 필요한지 계산할 수 있음.

3.3 가설검정

- 쓸데없는 토론에 종지부를 찍는 것
- ‘지금 우리가 수집할 수 있는 데이터의 범위 안에서’ 가설의 타당성에 초점을 맞춘다. 그 다음 내가 주장하는 가설을 완전히 번복하는 가설을 제시. 만약 내 주장을 완전히 번복하는 가설이 어느정도의 확률로 성립하는지를 얘기할 수 있다면, 상대가 내 주장을 완전히 번복하기는 어려움. 그 다음에 이를 “손해인가 이익인가”의 문제로 연결시킴.
